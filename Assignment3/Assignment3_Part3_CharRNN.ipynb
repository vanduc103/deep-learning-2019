{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 3: Language Modeling with CharRNN\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sang-gil Lee, October 2018\n",
    "\n",
    "This is a character-level language model using recurrent neural networks (RNNs).\n",
    "It has become very popular as a starter kit for learning how RNN works in practice.\n",
    "\n",
    "Before we start, what is \"language modeling\" anyway? Intuitively, \"language modeling\" is teaching the model about a general probability distribution of our words and sentences.\n",
    "\n",
    "So we ask the model like: \"hey just say whatever words from your estimation of the wikipedia word distribution\", and the model responds like \"ok, i learned from wikipedia, and the most frequent word is \"the\". so let me start with \"the\". the wikipedia is blah blah blah\"\n",
    "\n",
    "Thus, by teaching the model to speak for itself, we can test the model's capability of learning temporal relationships between sequences.\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/karpathy/char-rnn\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "But the original code is written in lua torch which looks less pretty :(\n",
    "\n",
    "There is a clean port of char-RNN in TensorFlow\n",
    "https://github.com/sherjilozair/char-rnn-tensorflow\n",
    "This iPython notebook is basically a copypasta of this repo.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "### AND MOST IMPORTANTLY, IF YOU JUST BLINDLY COPY PASTE THE CODE, YOU SHALL RUIN YOUR EXAM.\n",
    "### The exam is designed to be solvable for students that actually have written the code themselves.\n",
    "At least strictly re-type the codes from the original repo line-by-line, and understand what each line means thoroughly.\n",
    "\n",
    "## YOU HAVE BEEN WARNED. :)\n",
    "\n",
    "\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-5**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Character language modeling (20 points)\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. implementing **1. \\_\\_init\\_\\_()** and **2. sample()** of RNN **Model()** class from **char_rnn.py**\n",
    "\n",
    "2. briefly summarizing, at the end of the script, how you implmeneted the model & why you changed some other parts of the code. yes,  <font color=red> there are other intentional pitfalls inside the code </font>. just copy-pasting the \\_\\_init\\_\\_() will not work. can you tell me why?\n",
    "\n",
    "### The Definition of **\"work\"** is as follows:\n",
    "\n",
    "1. Training loss must be <font color=red> below 0.2 </font>. We will check the training loss output from the training code block. We don't split the data into train-valid-test. Don't forget to <font color=red> NOT clear the output from train(args)</font>, where the training loss will be printed! TA will check the logged output from train(args)\n",
    "\n",
    "2. calling sample(args.sample) at the last code block <font color=red> must generate some meaningful sentences </font>. The quality of the sentence does not count, unless the generated sentence is something like \"aaaaaaaaaaaaaabbbbbb\" or \"b\" u tlttfcwaU c  fGcnrh i.\\nh mt he!bsthpme\".\n",
    "\n",
    "\n",
    "\n",
    "Now proceed to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# ipython magic function for limiting the gpu to be seen for tensorflow\n",
    "# if you have just 1 GPU, specify the value to 0\n",
    "# if you have multiple GPUs (nut) and want to specify which GPU to use, specify this value to 0 or 1 or etc.\n",
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TensorFlow vram efficiency: if this is not specified, the model hogs all the VRAM even if it's not necessary\n",
    "# bad & greedy TF! but it has a reason for this design choice FWIW, try googling it if interested\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=5, data_dir='data/tinyshakespeare', decay_rate=0.7, grad_clip=20.0, init_from=None, input_keep_prob=0.1, learning_rate=0.01, model='lstm', num_epochs=50, num_layers=5, output_keep_prob=0.1, rnn_size=128, save_dir='models_char_rnn', save_every=1000, seq_length=500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing\n",
    "parser = argparse.ArgumentParser(\n",
    "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',\n",
    "                    help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='directory to store checkpointed models')\n",
    "parser.add_argument('--save_every', type=int, default=1000,\n",
    "                    help='Save frequency. Number of passes between checkpoints of the model.')\n",
    "parser.add_argument('--init_from', type=str, default=None,\n",
    "                    help=\"\"\"continue training from saved model at this path (usually \"save\").\n",
    "                        Path must contain files saved by previous training process:\n",
    "                        'config.pkl'        : configuration;\n",
    "                        'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                        'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                              Note: this file contains absolute paths, be careful when moving files around;\n",
    "                        'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                         Model params must be the same between multiple runs (model, rnn_size, num_layers and seq_length).\n",
    "                    \"\"\")\n",
    "# Model params\n",
    "parser.add_argument('--model', type=str, default='lstm',\n",
    "                    help='lstm, rnn, gru, or nas')\n",
    "parser.add_argument('--rnn_size', type=int, default=128,\n",
    "                    help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=5,\n",
    "                    help='number of layers in the RNN')\n",
    "# Optimization\n",
    "parser.add_argument('--seq_length', type=int, default=500,\n",
    "                    help='RNN sequence length. Number of timesteps to unroll for.')\n",
    "parser.add_argument('--batch_size', type=int, default=5,\n",
    "                    help=\"\"\"minibatch size. Number of sequences propagated through the network in parallel.\n",
    "                            Pick batch-sizes to fully leverage the GPU (e.g. until the memory is filled up)\n",
    "                            commonly in the range 10-500.\"\"\")\n",
    "parser.add_argument('--num_epochs', type=int, default=50,\n",
    "                    help='number of epochs. Number of full passes through the training examples.')\n",
    "parser.add_argument('--grad_clip', type=float, default=20.,\n",
    "                    help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.01,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.7,\n",
    "                    help='decay rate for rmsprop')\n",
    "parser.add_argument('--output_keep_prob', type=float, default=0.1,\n",
    "                    help='probability of keeping weights in the hidden layer')\n",
    "parser.add_argument('--input_keep_prob', type=float, default=0.1,\n",
    "                    help='probability of keeping weights in the input layer')\n",
    "\n",
    "# needed for argparsing within jupyter notebook\n",
    "# https://stackoverflow.com/questions/30656777/how-to-call-module-written-with-argparse-in-ipython-notebook\n",
    "sys.argv = ['-f']\n",
    "args = parser.parse_args()\n",
    "\n",
    "# print args: see if the hyperparemeters look pretty to you\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "[[49  9  7 ... 50  3 13]\n",
      " [ 0  5  1 ... 19  9  2]\n",
      " [24  0  3 ...  6 23  3]\n",
      " ...\n",
      " [ 7  9 27 ... 13  2  0]\n",
      " [ 4  2  0 ...  0  8  3]\n",
      " [11  4 19 ... 30  6  0]]\n",
      "(50, 100)\n",
      "[[ 9  7  6 ...  3 13  0]\n",
      " [ 5  1  0 ...  9  2 15]\n",
      " [ 0  3  7 ... 23  3  9]\n",
      " ...\n",
      " [ 9 27  4 ...  2  0  2]\n",
      " [ 2  0 12 ...  8  3  2]\n",
      " [ 4 19  1 ...  6  0 11]]\n",
      "(50, 100)\n"
     ]
    }
   ],
   "source": [
    "# protip: always check the data and poke around the data yourself\n",
    "# you will get a lot of insights by looking at the data\n",
    "args.seq_length=100\n",
    "data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "data_loader.reset_batch_pointer()\n",
    "\n",
    "x, y = data_loader.next_batch()\n",
    "\n",
    "# our data has a shape of (N, T), where N=batch_size and T=seq_length\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1\n",
      "  0 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1\n",
      "  4  7  0 14  1  0  6 23  1  4 28 25 10 10 26 11 11 24 10 35 23  1  4 28\n",
      " 16  0  6 23  1  4 28 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24\n",
      " 10 50  3 13]\n",
      "[ 9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1  0\n",
      " 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1  4\n",
      "  7  0 14  1  0  6 23  1  4 28 25 10 10 26 11 11 24 10 35 23  1  4 28 16\n",
      "  0  6 23  1  4 28 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10\n",
      " 50  3 13  0]\n"
     ]
    }
   ],
   "source": [
    "# see what the first entry of the batch look like\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "# y is just an x shifted to the left by one: so the network will predict the next token y given x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop definition\n",
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    print(\"vocabulary size: \" + str(args.vocab_size))\n",
    "\n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        # check if all necessary files exist\n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.latest_checkpoint(args.init_from)\n",
    "        assert ckpt, \"No checkpoint found\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same = [\"model\", \"rnn_size\", \"num_layers\", \"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "\n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagree on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagree on dictionary mappings!\"\n",
    "\n",
    "    if not os.path.isdir(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "    \n",
    "    print(\"building the model... may take some time...\")\n",
    "    ##################### This line builds the CharRNN model defined in char_rnn.py #####################\n",
    "    # set new args values\n",
    "    args.model = 'nas'\n",
    "    args.rnn_size = 256\n",
    "    args.num_layers = 2\n",
    "    args.seq_length = 100\n",
    "    args.batch_size = 50\n",
    "    args.num_epochs = 50\n",
    "    args.grad_clip = 5.0\n",
    "    args.learning_rate = 2e-3\n",
    "    args.decay_rate = 0.97\n",
    "    args.output_keep_prob = 1.0\n",
    "    args.input_keep_prob = 1.0\n",
    "    tf.reset_default_graph()\n",
    "    model = Model(args)\n",
    "    print(\"model built! starting training...\")\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt)\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr,\n",
    "                               args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state)\n",
    "            \n",
    "            for b in range(int(data_loader.num_batches)):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                for i, (c, h) in enumerate(model.initial_state):\n",
    "                    feed[c] = state[i].c\n",
    "                    feed[h] = state[i].h\n",
    "\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "\n",
    "                end = time.time()\n",
    "                \n",
    "                # print training log every 100 steps\n",
    "                if ((e * data_loader.num_batches + b) % 100 == 0):\n",
    "                    print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                          .format(e * data_loader.num_batches + b,\n",
    "                                  args.num_epochs * data_loader.num_batches,\n",
    "                                  e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                        or (e == args.num_epochs-1 and\n",
    "                            b == data_loader.num_batches-1):\n",
    "                    # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path,\n",
    "                               global_step=e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "building the model... may take some time...\n",
      "model built! starting training...\n",
      "0/11150 (epoch 0), train_loss = 4.181, time/batch = 1.186\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "100/11150 (epoch 0), train_loss = 2.430, time/batch = 0.204\n",
      "200/11150 (epoch 0), train_loss = 2.073, time/batch = 0.206\n",
      "300/11150 (epoch 1), train_loss = 1.885, time/batch = 0.206\n",
      "400/11150 (epoch 1), train_loss = 1.737, time/batch = 0.205\n",
      "500/11150 (epoch 2), train_loss = 1.653, time/batch = 0.206\n",
      "600/11150 (epoch 2), train_loss = 1.654, time/batch = 0.204\n",
      "700/11150 (epoch 3), train_loss = 1.586, time/batch = 0.206\n",
      "800/11150 (epoch 3), train_loss = 1.529, time/batch = 0.206\n",
      "900/11150 (epoch 4), train_loss = 1.529, time/batch = 0.207\n",
      "1000/11150 (epoch 4), train_loss = 1.475, time/batch = 0.207\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "1100/11150 (epoch 4), train_loss = 1.450, time/batch = 0.208\n",
      "1200/11150 (epoch 5), train_loss = 1.477, time/batch = 0.205\n",
      "1300/11150 (epoch 5), train_loss = 1.447, time/batch = 0.209\n",
      "1400/11150 (epoch 6), train_loss = 1.456, time/batch = 0.207\n",
      "1500/11150 (epoch 6), train_loss = 1.391, time/batch = 0.206\n",
      "1600/11150 (epoch 7), train_loss = 1.404, time/batch = 0.206\n",
      "1700/11150 (epoch 7), train_loss = 1.367, time/batch = 0.208\n",
      "1800/11150 (epoch 8), train_loss = 1.356, time/batch = 0.207\n",
      "1900/11150 (epoch 8), train_loss = 1.390, time/batch = 0.206\n",
      "2000/11150 (epoch 8), train_loss = 1.361, time/batch = 0.202\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "2100/11150 (epoch 9), train_loss = 1.350, time/batch = 0.206\n",
      "2200/11150 (epoch 9), train_loss = 1.338, time/batch = 0.200\n",
      "2300/11150 (epoch 10), train_loss = 1.347, time/batch = 0.205\n",
      "2400/11150 (epoch 10), train_loss = 1.311, time/batch = 0.205\n",
      "2500/11150 (epoch 11), train_loss = 1.330, time/batch = 0.207\n",
      "2600/11150 (epoch 11), train_loss = 1.325, time/batch = 0.206\n",
      "2700/11150 (epoch 12), train_loss = 1.264, time/batch = 0.223\n",
      "2800/11150 (epoch 12), train_loss = 1.313, time/batch = 0.205\n",
      "2900/11150 (epoch 13), train_loss = 1.312, time/batch = 0.207\n",
      "3000/11150 (epoch 13), train_loss = 1.262, time/batch = 0.206\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "3100/11150 (epoch 13), train_loss = 1.297, time/batch = 0.205\n",
      "3200/11150 (epoch 14), train_loss = 1.279, time/batch = 0.206\n",
      "3300/11150 (epoch 14), train_loss = 1.320, time/batch = 0.214\n",
      "3400/11150 (epoch 15), train_loss = 1.210, time/batch = 0.205\n",
      "3500/11150 (epoch 15), train_loss = 1.283, time/batch = 0.206\n",
      "3600/11150 (epoch 16), train_loss = 1.239, time/batch = 0.206\n",
      "3700/11150 (epoch 16), train_loss = 1.272, time/batch = 0.203\n",
      "3800/11150 (epoch 17), train_loss = 1.253, time/batch = 0.211\n",
      "3900/11150 (epoch 17), train_loss = 1.236, time/batch = 0.221\n",
      "4000/11150 (epoch 17), train_loss = 1.268, time/batch = 0.208\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "4100/11150 (epoch 18), train_loss = 1.257, time/batch = 0.204\n",
      "4200/11150 (epoch 18), train_loss = 1.248, time/batch = 0.207\n",
      "4300/11150 (epoch 19), train_loss = 1.248, time/batch = 0.208\n",
      "4400/11150 (epoch 19), train_loss = 1.230, time/batch = 0.206\n",
      "4500/11150 (epoch 20), train_loss = 1.202, time/batch = 0.206\n",
      "4600/11150 (epoch 20), train_loss = 1.233, time/batch = 0.213\n",
      "4700/11150 (epoch 21), train_loss = 1.222, time/batch = 0.207\n",
      "4800/11150 (epoch 21), train_loss = 1.248, time/batch = 0.206\n",
      "4900/11150 (epoch 21), train_loss = 1.206, time/batch = 0.205\n",
      "5000/11150 (epoch 22), train_loss = 1.189, time/batch = 0.209\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "5100/11150 (epoch 22), train_loss = 1.188, time/batch = 0.207\n",
      "5200/11150 (epoch 23), train_loss = 1.167, time/batch = 0.204\n",
      "5300/11150 (epoch 23), train_loss = 1.214, time/batch = 0.209\n",
      "5400/11150 (epoch 24), train_loss = 1.220, time/batch = 0.205\n",
      "5500/11150 (epoch 24), train_loss = 1.208, time/batch = 0.208\n",
      "5600/11150 (epoch 25), train_loss = 1.168, time/batch = 0.207\n",
      "5700/11150 (epoch 25), train_loss = 1.179, time/batch = 0.208\n",
      "5800/11150 (epoch 26), train_loss = 1.198, time/batch = 0.202\n",
      "5900/11150 (epoch 26), train_loss = 1.171, time/batch = 0.205\n",
      "6000/11150 (epoch 26), train_loss = 1.198, time/batch = 0.207\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "6100/11150 (epoch 27), train_loss = 1.221, time/batch = 0.208\n",
      "6200/11150 (epoch 27), train_loss = 1.169, time/batch = 0.202\n",
      "6300/11150 (epoch 28), train_loss = 1.190, time/batch = 0.204\n",
      "6400/11150 (epoch 28), train_loss = 1.203, time/batch = 0.206\n",
      "6500/11150 (epoch 29), train_loss = 1.190, time/batch = 0.201\n",
      "6600/11150 (epoch 29), train_loss = 1.209, time/batch = 0.205\n",
      "6700/11150 (epoch 30), train_loss = 1.138, time/batch = 0.226\n",
      "6800/11150 (epoch 30), train_loss = 1.177, time/batch = 0.204\n",
      "6900/11150 (epoch 30), train_loss = 1.153, time/batch = 0.201\n",
      "7000/11150 (epoch 31), train_loss = 1.183, time/batch = 0.205\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "7100/11150 (epoch 31), train_loss = 1.158, time/batch = 0.202\n",
      "7200/11150 (epoch 32), train_loss = 1.186, time/batch = 0.207\n",
      "7300/11150 (epoch 32), train_loss = 1.191, time/batch = 0.210\n",
      "7400/11150 (epoch 33), train_loss = 1.156, time/batch = 0.209\n",
      "7500/11150 (epoch 33), train_loss = 1.135, time/batch = 0.205\n",
      "7600/11150 (epoch 34), train_loss = 1.198, time/batch = 0.206\n",
      "7700/11150 (epoch 34), train_loss = 1.150, time/batch = 0.206\n",
      "7800/11150 (epoch 34), train_loss = 1.129, time/batch = 0.203\n",
      "7900/11150 (epoch 35), train_loss = 1.143, time/batch = 0.206\n",
      "8000/11150 (epoch 35), train_loss = 1.120, time/batch = 0.205\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "8100/11150 (epoch 36), train_loss = 1.153, time/batch = 0.204\n",
      "8200/11150 (epoch 36), train_loss = 1.133, time/batch = 0.207\n",
      "8300/11150 (epoch 37), train_loss = 1.132, time/batch = 0.200\n",
      "8400/11150 (epoch 37), train_loss = 1.173, time/batch = 0.205\n",
      "8500/11150 (epoch 38), train_loss = 1.124, time/batch = 0.204\n",
      "8600/11150 (epoch 38), train_loss = 1.140, time/batch = 0.204\n",
      "8700/11150 (epoch 39), train_loss = 1.150, time/batch = 0.216\n",
      "8800/11150 (epoch 39), train_loss = 1.163, time/batch = 0.201\n",
      "8900/11150 (epoch 39), train_loss = 1.151, time/batch = 0.205\n",
      "9000/11150 (epoch 40), train_loss = 1.165, time/batch = 0.207\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "9100/11150 (epoch 40), train_loss = 1.119, time/batch = 0.211\n",
      "9200/11150 (epoch 41), train_loss = 1.128, time/batch = 0.214\n",
      "9300/11150 (epoch 41), train_loss = 1.126, time/batch = 0.206\n",
      "9400/11150 (epoch 42), train_loss = 1.146, time/batch = 0.205\n",
      "9500/11150 (epoch 42), train_loss = 1.117, time/batch = 0.240\n",
      "9600/11150 (epoch 43), train_loss = 1.088, time/batch = 0.207\n",
      "9700/11150 (epoch 43), train_loss = 1.158, time/batch = 0.217\n",
      "9800/11150 (epoch 43), train_loss = 1.174, time/batch = 0.209\n",
      "9900/11150 (epoch 44), train_loss = 1.115, time/batch = 0.207\n",
      "10000/11150 (epoch 44), train_loss = 1.116, time/batch = 0.209\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "10100/11150 (epoch 45), train_loss = 1.095, time/batch = 0.207\n",
      "10200/11150 (epoch 45), train_loss = 1.099, time/batch = 0.205\n",
      "10300/11150 (epoch 46), train_loss = 1.136, time/batch = 0.210\n",
      "10400/11150 (epoch 46), train_loss = 1.109, time/batch = 0.205\n",
      "10500/11150 (epoch 47), train_loss = 1.128, time/batch = 0.207\n",
      "10600/11150 (epoch 47), train_loss = 1.116, time/batch = 0.206\n",
      "10700/11150 (epoch 47), train_loss = 1.158, time/batch = 0.205\n",
      "10800/11150 (epoch 48), train_loss = 1.111, time/batch = 0.237\n",
      "10900/11150 (epoch 48), train_loss = 1.133, time/batch = 0.208\n",
      "11000/11150 (epoch 49), train_loss = 1.107, time/batch = 0.205\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "11100/11150 (epoch 49), train_loss = 1.098, time/batch = 0.206\n",
      "model saved to models_char_rnn/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# let's train!\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars_vocab.pkl  config.pkl\t\t\t       model.ckpt-11149.index\r\n",
      "checkpoint\t model.ckpt-11149.data-00000-of-00001  model.ckpt-11149.meta\r\n"
     ]
    }
   ],
   "source": [
    "# we're done with the model. the weights are now safe inside our storage\n",
    "! ls {args.save_dir}\n",
    "\n",
    "# so begone all the ops, graphs & variables!\n",
    "# you may ask, why is this line needed? try commenting out the line and see what happens later in the sampling stage\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalulation\n",
    "\n",
    "<font color=red>**Your model could be evaluated without traning procedure,**</font> if you saved and loaded your model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sampling definition for evaluation phase\n",
    "# it uses the saved model and spit out some characters from the RNN model\n",
    "def sample_eval(args):\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    #Use most frequent char if no prime is given\n",
    "    if args.prime == '':\n",
    "        args.prime = chars[0]\n",
    "    model = Model(saved_args, training=False)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(str(model.sample(sess, chars, vocab, args.n, args.prime, sampling_type=2)),'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(n=500, prime='', save_dir='models_char_rnn')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing for sampling from the model\n",
    "parser_sample = argparse.ArgumentParser(\n",
    "                   formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser_sample.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='model directory to store checkpointed models')\n",
    "parser_sample.add_argument('-n', type=int, default=500,\n",
    "                    help='number of characters to sample')\n",
    "parser_sample.add_argument('--prime', type=text_type, default=u'',\n",
    "                    help='prime text')\n",
    "sys.argv = ['-f']\n",
    "args_sample = parser_sample.parse_args()\n",
    "args_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models_char_rnn/model.ckpt-11149\n",
      " Dond Thomas Clarence that our death?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Why, then I began his son we have found me in heaven,\n",
      "And set the holy reason knows not for death.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Why, sir, we will unto your grace hath been\n",
      "The man Paris like to his enemies.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Why, then can be content to dry like a day.\n",
      "\n",
      "KING RICHARD III:\n",
      "Stand by the law to his wife with my wife,\n",
      "And set the law makes a man and proud Angelo.\n",
      "\n",
      "BIANCA:\n",
      "Why, then now the Tower to look to Doctor,\n",
      "And he shall be so far graced and drea utf-8\n"
     ]
    }
   ],
   "source": [
    "# let's sample!\n",
    "sample_eval(args_sample)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Briefly summarize what & how you did, and why you did that way.\n",
    "This is also for checking yourself if you really learned something from this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CharRNN model:\n",
    "    - We use NAS RNN as based RNN model which we found giving a better result than LSTM model.\n",
    "- Other argument values are:\n",
    "    - rnn_size = hidden size of RNN cell = 256\n",
    "    - rnn_layers = 2 (use 2 rnn layers, if we use 5 layers then we have bad result)\n",
    "    - batch_size = 50, sequence length = 100 (input includes 100 chars), num_epoch = 50\n",
    "    - learning rate init = 2e-3 and decay rate is 0.97\n",
    "- Following are steps to build CharRNN model:\n",
    "    - Step 0: We declare multi layered rnn cell into one cell with dropout (use tensorflow DropoutWrapper and MultiRNNCell)\n",
    "    - Step 1: We declare input and target as tensors of (batch_size, sequence length). Initial state for RNN cells also initialized to zero state.\n",
    "    - Step 2: We transform input characters into an embedding matrix of (batch_size, rnn_size) (using tf.nn.embedding_lookup)\n",
    "    - Step 3: We unstack the input to fits in rnn model => (seq_length, batch_size, rnn_size)\n",
    "    - Step 4: We use a rnn_decoder (using tensorflow legacy_seq2seq.rnn_decoder) to generate the ouputs and final state (initial_state as above declared initial state)\n",
    "    - Step 5: Flatten the outputs from RNN cells and then use a softmax layer to create softmax outputs (to be able to compare with the targets)\n",
    "    - Step 6: Loss is calculate by the log loss and taking the average of the batch.\n",
    "    - Step 7: Calculate gradients and apply gradients for all trainable variables by using Adam Optimizer.\n",
    "- Training:\n",
    "    - We use a learning rate decay strategy by decaying learning rate every each epoch.\n",
    "    - But for best of our effort, the training loss only goes down to around 1.1, we cannot make it smaller 1.0.\n",
    "- Sampling:\n",
    "    - To sample a paragraph from a prime character, we feed consequently the input character into the trained model, get its output (as a softmax vector of vocab size) and the final state. The output softmax vector is then transformed into a sample character and feed into the model as next character input.\n",
    "    - The sampled output character can be taken by using highest probability (of softmax output) or by weighted probability pick if its previous character is a prime character or a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
