{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # one-hot encoding, Map the label 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 2000\n",
    "train_dataset = train_dataset[0:data_size]\n",
    "train_labels = train_labels[0:data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(train_dataset) # training set size\n",
    "nn_input_dim = 784 # input layer dimensionality\n",
    "nn_output_dim = 10 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_hdims = [100, 50]\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(nn_input_dim, nn_hdims[0]) / np.sqrt(nn_hdims[0]) # 784x1024\n",
    "b1 = np.zeros((1, nn_hdims[0])) # 1x1024\n",
    "W2 = np.random.randn(nn_hdims[0], nn_hdims[1]) / np.sqrt(nn_hdims[1]) # 1024x512\n",
    "b2 = np.zeros((1, nn_hdims[1])) # 1x512\n",
    "W3 = np.random.randn(nn_hdims[1], nn_output_dim) / np.sqrt(nn_output_dim) # 512x10\n",
    "b3 = np.zeros((1, nn_output_dim)) # 1x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(x):\n",
    "    return np.tanh(x)\n",
    "def derivative(x):\n",
    "    return 1.0 - np.tanh(x)*np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, loss value: 3.0610\n",
      "1000, loss value: 0.7320\n",
      "2000, loss value: 0.5782\n",
      "3000, loss value: 0.4941\n",
      "4000, loss value: 0.4364\n",
      "5000, loss value: 0.3931\n",
      "6000, loss value: 0.3751\n",
      "7000, loss value: 0.3590\n",
      "8000, loss value: 0.3445\n",
      "9000, loss value: 0.3315\n",
      "10000, loss value: 0.3197\n",
      "11000, loss value: 0.3142\n",
      "12000, loss value: 0.3090\n",
      "13000, loss value: 0.3040\n",
      "14000, loss value: 0.2992\n",
      "15000, loss value: 0.2947\n",
      "16000, loss value: 0.2925\n",
      "17000, loss value: 0.2903\n",
      "18000, loss value: 0.2882\n",
      "19000, loss value: 0.2862\n",
      "prediction accuracy: 87.15 %\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "num_examples = train_dataset.shape[0]\n",
    "iterations = 20000\n",
    "# training\n",
    "for l in range(iterations):\n",
    "    # forward\n",
    "    z1 = train_dataset.dot(W1) + b1 # Nx1024\n",
    "    a1 = np.tanh(z1) # Nx1024\n",
    "    z2 = a1.dot(W2) + b2 # Nx512\n",
    "    a2 = np.tanh(z2) # Nx512\n",
    "    z3 = a2.dot(W3) + b3 # Nx10\n",
    "    exp_scores = np.exp(z3)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis = 1, keepdims=True) # Nx10\n",
    "\n",
    "    # backward\n",
    "    dz3 = (probs - train_labels)/num_examples # Nx10 => error for 1 example\n",
    "    dW3 = (a2.T).dot(dz3) # 512x10\n",
    "    db3 = np.sum(dz3, axis = 0, keepdims=True) # 1x10\n",
    "\n",
    "    dz2 = (1-np.power(a2, 2)) * dz3.dot(W3.T) # Nx512\n",
    "    dW2 = (a1.T).dot(dz2) # 1024x512\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) # 1x512\n",
    "\n",
    "    dz1 = (1-np.power(a1, 2)) * dz2.dot(W2.T) # Nx1024\n",
    "    dW1 = (train_dataset.T).dot(dz1) # 784x1024\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) # 1x1024\n",
    "\n",
    "    # update\n",
    "    W1 -= epsilon * (dW1 + reg_lambda*W1)\n",
    "    W2 -= epsilon * (dW2 + reg_lambda*W2)\n",
    "    W3 -= epsilon * (dW3 + reg_lambda*W3)\n",
    "    b1 -= epsilon * db1\n",
    "    b2 -= epsilon * db2\n",
    "    b3 -= epsilon * db3\n",
    "    \n",
    "    # learning rate decay\n",
    "    if (l + 1) % 5000 == 0:\n",
    "        epsilon = epsilon * 0.5\n",
    "    \n",
    "    if l % 1000 == 0:\n",
    "        # loss\n",
    "        corect_logprobs = -np.log([probs[i,np.nonzero(train_labels)[(1)][i].astype('int64')] for i in range(num_examples)])\n",
    "        data_loss = np.sum(corect_logprobs)\n",
    "        # Add regulatization term to loss (optional)\n",
    "        data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "        loss = 1.0 / num_examples * data_loss # reduce to float\n",
    "        print(\"%d, loss value: %.4f\" % (l, loss))\n",
    "\n",
    "# testing\n",
    "z1 = test_dataset.dot(W1) + b1 # Nx1024\n",
    "a1 = np.tanh(z1) # Nx1024\n",
    "z2 = a1.dot(W2) + b2 # Nx512\n",
    "a2 = np.tanh(z2) # Nx512\n",
    "z3 = a2.dot(W3) + b3 # Nx10\n",
    "exp_scores = np.exp(z3)\n",
    "probs = exp_scores / np.sum(exp_scores, axis = 1, keepdims=True) # Nx10\n",
    "predict = np.argmax(probs, axis=1) # N\n",
    "true_labels = np.argmax(test_labels, axis=1) # N\n",
    "accuracy = np.sum(np.equal(predict, true_labels)) / test_dataset.shape[0]\n",
    "print(\"prediction accuracy: %.2f %%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, nn_input_dim])\n",
    "y_true = tf.placeholder(tf.float32, [None, nn_output_dim])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_hdims = [100, 50]\n",
    "W1 = tf.Variable(tf.truncated_normal(shape=[nn_input_dim, nn_hdims[0]]))\n",
    "b1 = tf.Variable(tf.zeros(shape=[nn_hdims[0]]))\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[nn_hdims[0], nn_hdims[1]]))\n",
    "b2 = tf.Variable(tf.zeros(shape=[nn_hdims[1]]))\n",
    "W3 = tf.Variable(tf.truncated_normal(shape=[nn_hdims[1], nn_output_dim]))\n",
    "b3 = tf.Variable(tf.zeros(shape=[nn_output_dim]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "reg_lambda = 1e-3\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=reg_lambda)\n",
    "#z1 = tf.matmul(x, W1) + b1\n",
    "#a1 = tf.tanh(z1)\n",
    "#z2 = tf.matmul(a1, W2) + b2\n",
    "#a2 = tf.nn.tanh(z2)\n",
    "z1 = tf.layers.dense(x, nn_hdims[0], activation=tf.nn.tanh, kernel_regularizer=regularizer)\n",
    "z1 = tf.nn.dropout(z1, keep_prob)\n",
    "z2 = tf.layers.dense(z1, nn_hdims[1], activation=tf.nn.tanh, kernel_regularizer=regularizer)\n",
    "z2 = tf.nn.dropout(z2, keep_prob)\n",
    "logits = tf.layers.dense(z2, nn_output_dim, kernel_regularizer=regularizer)\n",
    "pred = tf.nn.softmax(logits)\n",
    "pred_cls = tf.argmax(pred, axis=1)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = y_true)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.5\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           5000, 0.96, staircase=True)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = (\n",
    "    tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    .minimize(loss, global_step=global_step)\n",
    ")\n",
    "\n",
    "pred_accuracy = tf.equal(pred_cls, tf.argmax(y_true, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(pred_accuracy, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000, loss value: 0.0050\n",
      "2000, loss value: 0.0020\n",
      "3000, loss value: 0.0144\n",
      "4000, loss value: 0.0132\n",
      "5000, loss value: 0.0303\n",
      "6000, loss value: 0.0013\n",
      "7000, loss value: 0.0061\n",
      "8000, loss value: 0.0003\n",
      "9000, loss value: 0.0012\n",
      "10000, loss value: 0.0002\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_size = len(train_dataset)\n",
    "iterations = 10000\n",
    "session.run(tf.global_variables_initializer())\n",
    "for i in range(iterations):\n",
    "    offset = (i*batch_size) % (train_size - batch_size)\n",
    "    x_batch = train_dataset[offset : offset + batch_size, :]\n",
    "    y_batch = train_labels[offset : offset + batch_size, :]\n",
    "    \n",
    "    feed_dict_train = {x: x_batch, y_true: y_batch, keep_prob: 1.0}\n",
    "    _, cost = session.run([optimizer, loss], feed_dict=feed_dict_train)\n",
    "    \n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(\"%d, loss value: %.4f\" % (i+1, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 86.34 %\n"
     ]
    }
   ],
   "source": [
    "feed_dict_test = {x: test_dataset, y_true: test_labels, keep_prob: 1.0}\n",
    "acc = session.run(accuracy, feed_dict=feed_dict_test)\n",
    "print(\"prediction accuracy: %.2f %%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-19] *",
   "language": "python",
   "name": "conda-env-deep-learning-19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
