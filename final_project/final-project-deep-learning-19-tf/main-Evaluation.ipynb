{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Proejct: Text to Image Synthesis (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PPT file.**\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the training process **</font> so that TAs can grade both your code and results.  \n",
    "**The TA will set a config file as 'eval_birds.yml' when evaluating the code using 'hidden test dataset'. Thus, please make sure that your code can generate proper data to measure inception score and R-precision of 'hidden test dataset'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load datasets\n",
    "The Birds dataset will be downloaded automatically if it is not located in the *data* directory. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, nltk\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utils.data_utils import CUBDataset\n",
    "from utils.loss import cosine_similarity\n",
    "\n",
    "from utils.data_utils import *\n",
    "\n",
    "#################################################\n",
    "# DO NOT CHANGE \n",
    "from utils.model import CNN_ENCODER, RNN_ENCODER, GENERATOR, DISCRIMINATOR\n",
    "#################################################\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'BATCH_SIZE': 32,\n",
      " 'CHECKPOINT_DIR': './checkpoint',\n",
      " 'CHECKPOINT_NAME': 'model.ckpt',\n",
      " 'CNN': {'EMBEDDING_DIM': 0, 'H_DIM': 0},\n",
      " 'CONFIG_NAME': 'text-to-image',\n",
      " 'CUDA': False,\n",
      " 'DATASET_NAME': 'birds',\n",
      " 'DATA_DIR': 'data/birds',\n",
      " 'EMBEDDING_TYPE': 'cnn-rnn',\n",
      " 'GAN': {'B_ATTENTION': False,\n",
      "         'B_CONDITION': False,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 0,\n",
      "         'DF_DIM': 64,\n",
      "         'EMBEDDING_DIM': 0,\n",
      "         'GF_DIM': 128,\n",
      "         'R_NUM': 0,\n",
      "         'Z_DIM': 512},\n",
      " 'GPU_ID': '0',\n",
      " 'IMAGE_SIZE': 256,\n",
      " 'NUM_BATCH_FOR_TEST': 0,\n",
      " 'RANDOM_SEED': 0,\n",
      " 'RNN': {'EMBEDDING_DIM': 0,\n",
      "         'H_DIM': 256,\n",
      "         'TYPE': '',\n",
      "         'VOCAB_SIZE': 8000,\n",
      "         'WORD_EMBEDDING_DIM': 256},\n",
      " 'R_PRECISION_DIR': './evaluation',\n",
      " 'R_PRECISION_FILE': 'r_precision.npz',\n",
      " 'R_PRECISION_FILE_HIDDEN': 'r_precision_hidden.npz',\n",
      " 'TEST': {'B_EXAMPLE': False,\n",
      "          'GENERATED_HIDDEN_TEST_IMAGES': './evaluation/generated_images_hidden',\n",
      "          'GENERATED_TEST_IMAGES': './evaluation/generated_images'},\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 20},\n",
      " 'TRAIN': {'CNN_ENCODER': '',\n",
      "           'COEFF': {'COLOR_LOSS': 0.0, 'KL': 0.0, 'UNCOND_LOSS': 0.0},\n",
      "           'DISCRIMINATOR': '',\n",
      "           'DISCRIMINATOR_LR': 0.0,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR': '',\n",
      "           'GENERATOR_LR': 0.0,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'RNN_ENCODER': '',\n",
      "           'SNAPSHOT_INTERVAL': 0},\n",
      " 'WORKERS': 4,\n",
      " 'WRONG_CAPTION': 9}\n"
     ]
    }
   ],
   "source": [
    "# Set a config file as 'train_birds.yml' in training, as 'eval_birds.yml' for evaluation\n",
    "cfg_from_file('cfg/train_birds.yml') # eval_birds.yml\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3' #cfg.GPU_ID\n",
    "\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = 'sample/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf\n",
      "\n",
      "self.data_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds/CUB_200_2011\n",
      "\n",
      "Dataset already exists\n",
      "self.image_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds/CUB_200_2011/images\n",
      "\n",
      "Load from:  data/birds/captions.pickle\n",
      "self.current_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf\n",
      "\n",
      "self.data_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds/CUB_200_2011\n",
      "\n",
      "Dataset already exists\n",
      "self.image_dir:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds/CUB_200_2011/images\n",
      "\n",
      "Load from:  data/birds/captions.pickle\n",
      "\n",
      "train data directory:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds/train\n",
      "test data directory:\n",
      "/home/duclv/homework/deep-learning-2019/final_project/final-project-deep-learning-19-tf/data/birds/test\n",
      "\n",
      "# of train filenames:(8855,)\n",
      "# of test filenames:(2933,)\n",
      "\n",
      "example of filename of train image:002.Laysan_Albatross/Laysan_Albatross_0002_1027\n",
      "example of filename of valid image:001.Black_footed_Albatross/Black_Footed_Albatross_0046_18\n",
      "\n",
      "example of caption and its ids:\n",
      "['a', 'bird', 'with', 'a', 'very', 'long', 'wing', 'span', 'and', 'a', 'long', 'pointed', 'beak']\n",
      "[ 1  2  3  1  4  5  6  7  8  1  5  9 10  0  0  0  0  0  0  0]\n",
      "\n",
      "example of caption and its ids:\n",
      "['light', 'tan', 'colored', 'bird', 'with', 'a', 'white', 'head', 'and', 'an', 'orange', 'beak']\n",
      "[ 67 106  89   2   3   1  14  25   8  28  52  10   0   0   0   0   0   0\n",
      "   0   0]\n",
      "\n",
      "# of train captions:(88550,)\n",
      "# of test captions:(29330,)\n",
      "\n",
      "# of train caption ids:(88550, 20)\n",
      "# of test caption ids:(29330, 20)\n",
      "\n",
      "# of train images:(8855, 256, 256, 3)\n",
      "# of test images:(2933, 256, 256, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CUBDataset(cfg.DATA_DIR, split='train')\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, split='test')\n",
    "\n",
    "print(f'\\ntrain data directory:\\n{train_dataset.split_dir}')\n",
    "print(f'test data directory:\\n{test_dataset.split_dir}\\n')\n",
    "\n",
    "print(f'# of train filenames:{train_dataset.filenames.shape}')\n",
    "print(f'# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "\n",
    "print(f'example of filename of train image:{train_dataset.filenames[0]}')\n",
    "print(f'example of filename of valid image:{test_dataset.filenames[0]}\\n')\n",
    "\n",
    "print(f'example of caption and its ids:\\n{train_dataset.captions[0]}\\n{train_dataset.captions_ids[0]}\\n')\n",
    "print(f'example of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "\n",
    "print(f'# of train captions:{np.asarray(train_dataset.captions).shape}')\n",
    "print(f'# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "\n",
    "print(f'# of train caption ids:{np.asarray(train_dataset.captions_ids).shape}')\n",
    "print(f'# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "print(f'# of train images:{train_dataset.images.shape}')\n",
    "print(f'# of test images:{test_dataset.images.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define models and go to train/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###======================== DEFIINE VARIABLES ===================================###\n",
    "lr = 0.0002\n",
    "lr_decay = 0.5      \n",
    "decay_every = 100  \n",
    "beta1 = 0.5\n",
    "batch_size = cfg.BATCH_SIZE\n",
    "image_size = cfg.IMAGE_SIZE\n",
    "z_dim = cfg.GAN.Z_DIM\n",
    "\n",
    "tf.reset_default_graph()\n",
    "from importlib import reload\n",
    "import utils.model as model\n",
    "model = reload(model)\n",
    "RNN_ENCODER = model.RNN_ENCODER\n",
    "GENERATOR = model.GENERATOR\n",
    "DISCRIMINATOR = model.DISCRIMINATOR\n",
    "CNN_ENCODER = model.CNN_ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###======================== DEFIINE PLACEHOLDER ===================================###\n",
    "t_real_image = tf.placeholder('float32', [cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.IMAGE_SIZE, 3], name = 'real_image')\n",
    "t_real_caption = tf.placeholder(dtype=tf.int64, shape=[cfg.BATCH_SIZE , None], name='real_caption_input')\n",
    "t_wrong_image = tf.placeholder('float32', [cfg.BATCH_SIZE ,cfg.IMAGE_SIZE, cfg.IMAGE_SIZE, 3], name = 'wrong_image')\n",
    "t_wrong_caption = tf.placeholder(dtype=tf.int64, shape=[cfg.BATCH_SIZE , None], name='wrong_caption_input')\n",
    "t_z = tf.placeholder(tf.float32, [cfg.BATCH_SIZE , cfg.GAN.Z_DIM], name='z_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = RNN_ENCODER(t_real_caption, cfg.BATCH_SIZE, is_training=False, reuse=False)\n",
    "generator = GENERATOR(t_z, rnn_encoder.outputs, is_training=False, reuse=False)\n",
    "discriminator = DISCRIMINATOR(generator.outputs, rnn_encoder.outputs, is_training=False, reuse=False)\n",
    "cnn_encoder = CNN_ENCODER(t_real_image, is_training=False, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define image and text mapping\n",
    "net_cnn = CNN_ENCODER(t_real_image, is_training=True, reuse=True)\n",
    "x = net_cnn.outputs\n",
    "v = RNN_ENCODER(t_real_caption, cfg.BATCH_SIZE, is_training=True, reuse=True).outputs\n",
    "x_w = CNN_ENCODER(t_wrong_image, is_training=True, reuse=True).outputs\n",
    "v_w = RNN_ENCODER(t_wrong_caption, cfg.BATCH_SIZE, is_training=True, reuse=True).outputs\n",
    "\n",
    "alpha = 0.2 # margin alpha\n",
    "rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n",
    "            tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define generative model\n",
    "net_rnn = RNN_ENCODER(t_real_caption, cfg.BATCH_SIZE, is_training=False, reuse=True)\n",
    "net_fake_image = GENERATOR(t_z, net_rnn.outputs, is_training=True, reuse=True)\n",
    "\n",
    "net_disc_fake = DISCRIMINATOR(net_fake_image.outputs, net_rnn.outputs, is_training=True, reuse=True)\n",
    "disc_fake_logits = net_disc_fake.logits\n",
    "\n",
    "net_disc_real = DISCRIMINATOR(t_real_image, net_rnn.outputs, is_training=True, reuse=True)\n",
    "disc_real_logits = net_disc_real.logits\n",
    "\n",
    "net_disc_mismatch = DISCRIMINATOR(t_real_image, \n",
    "                            RNN_ENCODER(t_wrong_caption, cfg.BATCH_SIZE, is_training=False, reuse=True).outputs,\n",
    "                            is_training=True, reuse=True)\n",
    "disc_mismatch_logits = net_disc_mismatch.logits\n",
    "\n",
    "d_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real_logits,     labels=tf.ones_like(disc_real_logits),      name='d1'))\n",
    "d_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_mismatch_logits, labels=tf.zeros_like(disc_mismatch_logits), name='d2'))\n",
    "d_loss3 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits,     labels=tf.zeros_like(disc_fake_logits),     name='d3'))\n",
    "d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits, labels=tf.ones_like(disc_fake_logits), name='g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define optimzers\n",
    "rnn_vars = [var for var in tf.trainable_variables() if 'rnnencoder' in var.name]\n",
    "g_vars = [var for var in tf.trainable_variables() if 'generator' in var.name]\n",
    "d_vars = [var for var in tf.trainable_variables() if 'discriminator' in var.name]\n",
    "cnn_vars = [var for var in tf.trainable_variables() if 'cnnencoder' in var.name]\n",
    "\n",
    "update_ops_D = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'discriminator' in var.name]\n",
    "update_ops_G = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'generator' in var.name]\n",
    "update_ops_CNN = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'cnnencoder' in var.name]\n",
    "\n",
    "'''print('----------Update_ops_D--------')\n",
    "for var in update_ops_D:\n",
    "    print(var.name)\n",
    "print('----------Update_ops_G--------')\n",
    "for var in update_ops_G:\n",
    "    print(var.name)\n",
    "print('----------Update_ops_CNN--------')\n",
    "for var in update_ops_CNN:\n",
    "    print(var.name)'''\n",
    "\n",
    "with tf.variable_scope('learning_rate'):\n",
    "    lr_v = tf.Variable(lr, trainable=False)\n",
    "\n",
    "with tf.control_dependencies(update_ops_D):\n",
    "    d_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "with tf.control_dependencies(update_ops_G):\n",
    "    g_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "with tf.control_dependencies(update_ops_CNN):\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(rnn_loss, rnn_vars + cnn_vars), 10)\n",
    "    optimizer = tf.train.AdamOptimizer(lr_v, beta1=beta1)\n",
    "    rnn_optim = optimizer.apply_gradients(zip(grads, rnn_vars + cnn_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ** init lr: 0.000200  decay_every_epoch: 100, lr_decay: 0.500000\n",
      "Epoch: [ 0/600] [   0/ 276] time: 63.1723s, d_loss: 6.23484230, g_loss: 39.32595444, rnn_loss: 0.40280259\n",
      "Epoch: [ 0/600] [   1/ 276] time: 6.0927s, d_loss: 17.23238945, g_loss: 11.64991283, rnn_loss: 0.38357860\n",
      "Epoch: [ 0/600] [   2/ 276] time: 3.7340s, d_loss: 6.36725235, g_loss: 0.27039823, rnn_loss: 0.40155798\n",
      "Epoch: [ 0/600] [   3/ 276] time: 7.3573s, d_loss: 10.66509056, g_loss: 15.84979153, rnn_loss: 0.42965314\n",
      "Epoch: [ 0/600] [   4/ 276] time: 3.6836s, d_loss: 4.12254572, g_loss: 23.20690918, rnn_loss: 0.34749711\n",
      "Epoch: [ 0/600] [   5/ 276] time: 7.1800s, d_loss: 5.92113686, g_loss: 15.14709949, rnn_loss: 0.38436699\n",
      "Epoch: [ 0/600] [   6/ 276] time: 4.1041s, d_loss: 3.60993147, g_loss: 5.07561350, rnn_loss: 0.43422377\n",
      "Epoch: [ 0/600] [   7/ 276] time: 7.0240s, d_loss: 6.93212175, g_loss: 27.30155945, rnn_loss: 0.42405000\n",
      "Epoch: [ 0/600] [   8/ 276] time: 4.2256s, d_loss: 4.57340956, g_loss: 15.40741634, rnn_loss: 0.40224382\n",
      "Epoch: [ 0/600] [   9/ 276] time: 7.2312s, d_loss: 7.13530302, g_loss: 21.42374039, rnn_loss: 0.39271539\n",
      "Epoch: [ 0/600] [  10/ 276] time: 3.6335s, d_loss: 3.19075394, g_loss: 23.76106453, rnn_loss: 0.34432888\n",
      "Epoch: [ 0/600] [  11/ 276] time: 7.3010s, d_loss: 3.54095292, g_loss: 24.24390030, rnn_loss: 0.45775449\n",
      "Epoch: [ 0/600] [  12/ 276] time: 3.5447s, d_loss: 2.78453398, g_loss: 18.76749802, rnn_loss: 0.40784436\n",
      "Epoch: [ 0/600] [  13/ 276] time: 8.1612s, d_loss: 4.92043209, g_loss: 15.30222893, rnn_loss: 0.41065979\n",
      "Epoch: [ 0/600] [  14/ 276] time: 3.6702s, d_loss: 5.10057640, g_loss: 34.45763397, rnn_loss: 0.43081689\n",
      "Epoch: [ 0/600] [  15/ 276] time: 6.9753s, d_loss: 3.92119694, g_loss: 17.73288727, rnn_loss: 0.45350319\n",
      "Epoch: [ 0/600] [  16/ 276] time: 3.6871s, d_loss: 4.02029848, g_loss: 29.13679504, rnn_loss: 0.37852198\n",
      "Epoch: [ 0/600] [  17/ 276] time: 7.0535s, d_loss: 4.70899057, g_loss: 12.79083633, rnn_loss: 0.40384033\n",
      "Epoch: [ 0/600] [  18/ 276] time: 4.2545s, d_loss: 5.81979799, g_loss: 33.40040970, rnn_loss: 0.39707920\n",
      "Epoch: [ 0/600] [  19/ 276] time: 7.8639s, d_loss: 4.00588274, g_loss: 31.21527863, rnn_loss: 0.35251373\n",
      "Epoch: [ 0/600] [  20/ 276] time: 5.7830s, d_loss: 4.69097567, g_loss: 14.01128483, rnn_loss: 0.36878100\n",
      "Epoch: [ 0/600] [  21/ 276] time: 4.6647s, d_loss: 5.82053661, g_loss: 40.49610901, rnn_loss: 0.38727748\n",
      "Epoch: [ 0/600] [  22/ 276] time: 6.1316s, d_loss: 4.29665899, g_loss: 31.26020432, rnn_loss: 0.40791827\n",
      "Epoch: [ 0/600] [  23/ 276] time: 5.6172s, d_loss: 5.07569838, g_loss: 23.76620483, rnn_loss: 0.43389398\n",
      "Epoch: [ 0/600] [  24/ 276] time: 5.3324s, d_loss: 5.43643904, g_loss: 17.58444977, rnn_loss: 0.41018277\n",
      "Epoch: [ 0/600] [  25/ 276] time: 4.8894s, d_loss: 3.55126476, g_loss: 14.56382942, rnn_loss: 0.41688344\n",
      "Epoch: [ 0/600] [  26/ 276] time: 5.8255s, d_loss: 5.99760818, g_loss: 9.71729469, rnn_loss: 0.44659734\n",
      "Epoch: [ 0/600] [  27/ 276] time: 4.8183s, d_loss: 6.63201761, g_loss: 17.05675316, rnn_loss: 0.39721775\n",
      "Epoch: [ 0/600] [  28/ 276] time: 5.6992s, d_loss: 3.69363737, g_loss: 20.90710068, rnn_loss: 0.40237966\n",
      "Epoch: [ 0/600] [  29/ 276] time: 4.9018s, d_loss: 6.97352266, g_loss: 10.93231010, rnn_loss: 0.37543181\n",
      "Epoch: [ 0/600] [  30/ 276] time: 5.9915s, d_loss: 6.78491402, g_loss: 30.16638947, rnn_loss: 0.39937359\n",
      "Epoch: [ 0/600] [  31/ 276] time: 5.2829s, d_loss: 2.91650152, g_loss: 24.27602768, rnn_loss: 0.37625894\n",
      "Epoch: [ 0/600] [  32/ 276] time: 5.7783s, d_loss: 3.21504164, g_loss: 16.56888199, rnn_loss: 0.41756639\n",
      "Epoch: [ 0/600] [  33/ 276] time: 4.7524s, d_loss: 4.72268772, g_loss: 12.13331604, rnn_loss: 0.40395910\n",
      "Epoch: [ 0/600] [  34/ 276] time: 5.1753s, d_loss: 4.52412415, g_loss: 11.42285442, rnn_loss: 0.42367154\n",
      "Epoch: [ 0/600] [  35/ 276] time: 5.2383s, d_loss: 3.52442265, g_loss: 15.59801674, rnn_loss: 0.43162316\n",
      "Epoch: [ 0/600] [  36/ 276] time: 5.9303s, d_loss: 2.89960098, g_loss: 13.69252968, rnn_loss: 0.38643765\n",
      "Epoch: [ 0/600] [  37/ 276] time: 5.1792s, d_loss: 3.48283958, g_loss: 6.55382633, rnn_loss: 0.36359996\n",
      "Epoch: [ 0/600] [  38/ 276] time: 6.2230s, d_loss: 3.73394179, g_loss: 13.29688454, rnn_loss: 0.38706452\n",
      "Epoch: [ 0/600] [  39/ 276] time: 5.1594s, d_loss: 4.70306778, g_loss: 11.64121056, rnn_loss: 0.41999763\n",
      "Epoch: [ 0/600] [  40/ 276] time: 5.8721s, d_loss: 3.22805214, g_loss: 11.47614384, rnn_loss: 0.40052322\n",
      "Epoch: [ 0/600] [  41/ 276] time: 5.1733s, d_loss: 3.27885699, g_loss: 9.25668526, rnn_loss: 0.39300537\n",
      "Epoch: [ 0/600] [  42/ 276] time: 5.1996s, d_loss: 3.62683725, g_loss: 15.22969437, rnn_loss: 0.42344135\n",
      "Epoch: [ 0/600] [  43/ 276] time: 4.6108s, d_loss: 3.82429123, g_loss: 12.84944153, rnn_loss: 0.45451534\n",
      "Epoch: [ 0/600] [  44/ 276] time: 5.1189s, d_loss: 3.16420341, g_loss: 15.06090546, rnn_loss: 0.38775054\n",
      "Epoch: [ 0/600] [  45/ 276] time: 4.5260s, d_loss: 4.72673893, g_loss: 10.62887955, rnn_loss: 0.44889984\n",
      "Epoch: [ 0/600] [  46/ 276] time: 5.0862s, d_loss: 4.93896151, g_loss: 14.36263847, rnn_loss: 0.36700118\n",
      "Epoch: [ 0/600] [  47/ 276] time: 4.5346s, d_loss: 3.01303315, g_loss: 8.86478996, rnn_loss: 0.39918476\n",
      "Epoch: [ 0/600] [  48/ 276] time: 5.1996s, d_loss: 3.56047750, g_loss: 23.09468460, rnn_loss: 0.42005116\n",
      "Epoch: [ 0/600] [  49/ 276] time: 4.5884s, d_loss: 3.77242637, g_loss: 13.75115967, rnn_loss: 0.40010667\n",
      "Epoch: [ 0/600] [  50/ 276] time: 5.0812s, d_loss: 2.97405958, g_loss: 9.07986355, rnn_loss: 0.41572273\n",
      "Epoch: [ 0/600] [  51/ 276] time: 4.8483s, d_loss: 4.01043224, g_loss: 24.93882751, rnn_loss: 0.34364596\n",
      "Epoch: [ 0/600] [  52/ 276] time: 5.2086s, d_loss: 4.35507107, g_loss: 14.25860977, rnn_loss: 0.38577110\n",
      "Epoch: [ 0/600] [  53/ 276] time: 4.8614s, d_loss: 5.41648626, g_loss: 20.29014206, rnn_loss: 0.40974838\n",
      "Epoch: [ 0/600] [  54/ 276] time: 5.0549s, d_loss: 2.95780253, g_loss: 17.77516747, rnn_loss: 0.43672615\n",
      "Epoch: [ 0/600] [  55/ 276] time: 4.5814s, d_loss: 3.05912971, g_loss: 12.37930107, rnn_loss: 0.42391646\n",
      "Epoch: [ 0/600] [  56/ 276] time: 5.2510s, d_loss: 3.07984972, g_loss: 12.91402054, rnn_loss: 0.40677828\n",
      "Epoch: [ 0/600] [  57/ 276] time: 5.3121s, d_loss: 2.59225178, g_loss: 14.41803646, rnn_loss: 0.41264123\n",
      "Epoch: [ 0/600] [  58/ 276] time: 5.4264s, d_loss: 2.74357247, g_loss: 16.95342445, rnn_loss: 0.39763579\n",
      "Epoch: [ 0/600] [  59/ 276] time: 4.5261s, d_loss: 4.02251768, g_loss: 6.85478449, rnn_loss: 0.36269075\n",
      "Epoch: [ 0/600] [  60/ 276] time: 5.2545s, d_loss: 3.52798033, g_loss: 28.55873299, rnn_loss: 0.38648036\n",
      "Epoch: [ 0/600] [  61/ 276] time: 4.5681s, d_loss: 2.63805532, g_loss: 28.35417557, rnn_loss: 0.39438063\n",
      "Epoch: [ 0/600] [  62/ 276] time: 5.1819s, d_loss: 2.44669771, g_loss: 20.34533691, rnn_loss: 0.38579008\n",
      "Epoch: [ 0/600] [  63/ 276] time: 4.8767s, d_loss: 3.91805387, g_loss: 12.40219593, rnn_loss: 0.38113344\n",
      "Epoch: [ 0/600] [  64/ 276] time: 5.1638s, d_loss: 3.27831411, g_loss: 9.34135342, rnn_loss: 0.36177704\n",
      "Epoch: [ 0/600] [  65/ 276] time: 4.6205s, d_loss: 3.06987023, g_loss: 11.01096725, rnn_loss: 0.38038364\n",
      "Epoch: [ 0/600] [  66/ 276] time: 5.2098s, d_loss: 2.93215609, g_loss: 8.82036781, rnn_loss: 0.42741978\n",
      "Epoch: [ 0/600] [  67/ 276] time: 5.1628s, d_loss: 3.97929096, g_loss: 7.60714674, rnn_loss: 0.42354256\n",
      "Epoch: [ 0/600] [  68/ 276] time: 5.8137s, d_loss: 3.43303514, g_loss: 13.54404259, rnn_loss: 0.39073914\n",
      "Epoch: [ 0/600] [  69/ 276] time: 5.1491s, d_loss: 2.53239775, g_loss: 11.30351257, rnn_loss: 0.35559428\n",
      "Epoch: [ 0/600] [  70/ 276] time: 5.6413s, d_loss: 4.59240341, g_loss: 16.35903740, rnn_loss: 0.40371349\n",
      "Epoch: [ 0/600] [  71/ 276] time: 5.1657s, d_loss: 5.58509684, g_loss: 12.06699085, rnn_loss: 0.38148320\n",
      "Epoch: [ 0/600] [  72/ 276] time: 5.8541s, d_loss: 2.76937199, g_loss: 12.54654503, rnn_loss: 0.39300305\n",
      "Epoch: [ 0/600] [  73/ 276] time: 5.6714s, d_loss: 2.72304869, g_loss: 12.09609509, rnn_loss: 0.38220769\n",
      "Epoch: [ 0/600] [  74/ 276] time: 5.8907s, d_loss: 4.26399422, g_loss: 11.42292786, rnn_loss: 0.35862973\n",
      "Epoch: [ 0/600] [  75/ 276] time: 5.1548s, d_loss: 3.30605698, g_loss: 10.29154205, rnn_loss: 0.41568461\n",
      "Epoch: [ 0/600] [  76/ 276] time: 5.6925s, d_loss: 4.64623737, g_loss: 11.24318218, rnn_loss: 0.42478693\n",
      "Epoch: [ 0/600] [  77/ 276] time: 5.1252s, d_loss: 2.47255039, g_loss: 9.09716511, rnn_loss: 0.40328878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0/600] [  78/ 276] time: 5.5845s, d_loss: 2.58349800, g_loss: 9.64740562, rnn_loss: 0.37367791\n",
      "Epoch: [ 0/600] [  79/ 276] time: 5.7236s, d_loss: 3.73621202, g_loss: 5.84757996, rnn_loss: 0.35291892\n",
      "Epoch: [ 0/600] [  80/ 276] time: 6.0420s, d_loss: 2.27605391, g_loss: 7.13578415, rnn_loss: 0.41712159\n",
      "Epoch: [ 0/600] [  81/ 276] time: 5.3392s, d_loss: 3.03734398, g_loss: 6.15043831, rnn_loss: 0.41445935\n",
      "Epoch: [ 0/600] [  82/ 276] time: 6.0953s, d_loss: 2.76614571, g_loss: 8.06743908, rnn_loss: 0.41888702\n",
      "Epoch: [ 0/600] [  83/ 276] time: 5.3929s, d_loss: 2.06157708, g_loss: 7.65023899, rnn_loss: 0.36965406\n",
      "Epoch: [ 0/600] [  84/ 276] time: 6.1096s, d_loss: 2.72530508, g_loss: 8.16736984, rnn_loss: 0.38652378\n",
      "Epoch: [ 0/600] [  85/ 276] time: 5.1447s, d_loss: 2.39750481, g_loss: 9.42800808, rnn_loss: 0.42467290\n",
      "Epoch: [ 0/600] [  86/ 276] time: 5.6534s, d_loss: 2.35646534, g_loss: 8.01881599, rnn_loss: 0.41260284\n",
      "Epoch: [ 0/600] [  87/ 276] time: 4.7323s, d_loss: 3.04952931, g_loss: 4.40872669, rnn_loss: 0.46026492\n",
      "Epoch: [ 0/600] [  88/ 276] time: 5.8773s, d_loss: 2.54973578, g_loss: 7.35895586, rnn_loss: 0.43618539\n",
      "Epoch: [ 0/600] [  89/ 276] time: 5.3951s, d_loss: 3.76145673, g_loss: 3.31009150, rnn_loss: 0.38167757\n",
      "Epoch: [ 0/600] [  90/ 276] time: 6.1795s, d_loss: 2.58516788, g_loss: 8.72196770, rnn_loss: 0.44489038\n",
      "Epoch: [ 0/600] [  91/ 276] time: 5.7402s, d_loss: 2.22713089, g_loss: 9.00003815, rnn_loss: 0.42525399\n",
      "Epoch: [ 0/600] [  92/ 276] time: 6.0863s, d_loss: 2.70194697, g_loss: 5.52852821, rnn_loss: 0.39249742\n",
      "Epoch: [ 0/600] [  93/ 276] time: 5.2084s, d_loss: 2.21291590, g_loss: 8.58159828, rnn_loss: 0.40373933\n",
      "Epoch: [ 0/600] [  94/ 276] time: 6.0281s, d_loss: 2.45621967, g_loss: 3.41205668, rnn_loss: 0.41466993\n",
      "Epoch: [ 0/600] [  95/ 276] time: 6.2612s, d_loss: 2.41324162, g_loss: 9.54131889, rnn_loss: 0.38999605\n",
      "Epoch: [ 0/600] [  96/ 276] time: 5.9217s, d_loss: 2.12436152, g_loss: 7.72421265, rnn_loss: 0.39265987\n",
      "Epoch: [ 0/600] [  97/ 276] time: 5.1328s, d_loss: 1.65084171, g_loss: 5.46155596, rnn_loss: 0.40022749\n",
      "Epoch: [ 0/600] [  98/ 276] time: 6.1052s, d_loss: 2.41147447, g_loss: 3.62372661, rnn_loss: 0.42982689\n",
      "Epoch: [ 0/600] [  99/ 276] time: 5.2627s, d_loss: 4.03245354, g_loss: 7.73444462, rnn_loss: 0.37981188\n",
      "Epoch: [ 0/600] [ 100/ 276] time: 5.2037s, d_loss: 2.81128860, g_loss: 5.08282375, rnn_loss: 0.39389935\n",
      "Epoch: [ 0/600] [ 101/ 276] time: 4.8000s, d_loss: 2.91453171, g_loss: 6.66944408, rnn_loss: 0.41151917\n",
      "Epoch: [ 0/600] [ 102/ 276] time: 5.7936s, d_loss: 2.86237764, g_loss: 5.08527756, rnn_loss: 0.40042913\n",
      "Epoch: [ 0/600] [ 103/ 276] time: 5.3927s, d_loss: 2.44357324, g_loss: 9.20031452, rnn_loss: 0.38005581\n",
      "Epoch: [ 0/600] [ 104/ 276] time: 6.0229s, d_loss: 4.97060919, g_loss: 1.04140925, rnn_loss: 0.36904889\n",
      "Epoch: [ 0/600] [ 105/ 276] time: 5.0520s, d_loss: 3.48108983, g_loss: 7.40253401, rnn_loss: 0.33953106\n",
      "Epoch: [ 0/600] [ 106/ 276] time: 5.1645s, d_loss: 2.05636692, g_loss: 6.71625423, rnn_loss: 0.39460731\n",
      "Epoch: [ 0/600] [ 107/ 276] time: 5.1744s, d_loss: 2.65121365, g_loss: 4.62554741, rnn_loss: 0.39935920\n",
      "Epoch: [ 0/600] [ 108/ 276] time: 5.6568s, d_loss: 3.05252695, g_loss: 5.20034218, rnn_loss: 0.34900919\n",
      "Epoch: [ 0/600] [ 109/ 276] time: 4.7185s, d_loss: 2.99718523, g_loss: 6.25288105, rnn_loss: 0.42663863\n",
      "Epoch: [ 0/600] [ 110/ 276] time: 6.0612s, d_loss: 1.99215376, g_loss: 3.57057142, rnn_loss: 0.40743369\n",
      "Epoch: [ 0/600] [ 111/ 276] time: 4.8780s, d_loss: 2.18329883, g_loss: 7.57964802, rnn_loss: 0.40904802\n",
      "Epoch: [ 0/600] [ 112/ 276] time: 5.6542s, d_loss: 2.86601353, g_loss: 3.51943111, rnn_loss: 0.39844993\n",
      "Epoch: [ 0/600] [ 113/ 276] time: 5.8084s, d_loss: 2.32161403, g_loss: 7.79383278, rnn_loss: 0.36305559\n",
      "Epoch: [ 0/600] [ 114/ 276] time: 5.7751s, d_loss: 2.69888830, g_loss: 6.37810373, rnn_loss: 0.40285760\n",
      "Epoch: [ 0/600] [ 115/ 276] time: 5.0332s, d_loss: 2.41812897, g_loss: 5.30137777, rnn_loss: 0.37543193\n",
      "Epoch: [ 0/600] [ 116/ 276] time: 5.1260s, d_loss: 1.72930062, g_loss: 7.93109751, rnn_loss: 0.41094464\n",
      "Epoch: [ 0/600] [ 117/ 276] time: 5.1762s, d_loss: 2.56020522, g_loss: 5.02097082, rnn_loss: 0.40741110\n",
      "Epoch: [ 0/600] [ 118/ 276] time: 5.6506s, d_loss: 1.79555202, g_loss: 3.24139619, rnn_loss: 0.40451497\n",
      "Epoch: [ 0/600] [ 119/ 276] time: 4.7526s, d_loss: 2.10668921, g_loss: 5.37731838, rnn_loss: 0.40653569\n",
      "Epoch: [ 0/600] [ 120/ 276] time: 5.8417s, d_loss: 2.02922821, g_loss: 3.07833672, rnn_loss: 0.39302620\n",
      "Epoch: [ 0/600] [ 121/ 276] time: 5.1992s, d_loss: 2.42647958, g_loss: 4.77738953, rnn_loss: 0.37738228\n",
      "Epoch: [ 0/600] [ 122/ 276] time: 5.9350s, d_loss: 2.32575560, g_loss: 2.17497087, rnn_loss: 0.42358556\n",
      "Epoch: [ 0/600] [ 123/ 276] time: 4.9875s, d_loss: 2.20331264, g_loss: 4.15009022, rnn_loss: 0.34728837\n",
      "Epoch: [ 0/600] [ 124/ 276] time: 5.9988s, d_loss: 2.03761697, g_loss: 4.93501186, rnn_loss: 0.42586678\n",
      "Epoch: [ 0/600] [ 125/ 276] time: 4.8503s, d_loss: 2.22653413, g_loss: 1.69532347, rnn_loss: 0.35870582\n",
      "Epoch: [ 0/600] [ 126/ 276] time: 5.7455s, d_loss: 2.43273544, g_loss: 4.28461742, rnn_loss: 0.38690779\n",
      "Epoch: [ 0/600] [ 127/ 276] time: 5.8253s, d_loss: 2.00543308, g_loss: 3.37829423, rnn_loss: 0.45309791\n",
      "Epoch: [ 0/600] [ 128/ 276] time: 5.7075s, d_loss: 1.67312884, g_loss: 5.32025337, rnn_loss: 0.35300469\n",
      "Epoch: [ 0/600] [ 129/ 276] time: 4.7104s, d_loss: 2.02758312, g_loss: 4.24571562, rnn_loss: 0.37241000\n",
      "Epoch: [ 0/600] [ 130/ 276] time: 5.9738s, d_loss: 1.99371719, g_loss: 2.49399495, rnn_loss: 0.40796676\n",
      "Epoch: [ 0/600] [ 131/ 276] time: 5.5459s, d_loss: 2.79171991, g_loss: 4.80256939, rnn_loss: 0.40068379\n",
      "Epoch: [ 0/600] [ 132/ 276] time: 6.1484s, d_loss: 1.65342021, g_loss: 6.47636604, rnn_loss: 0.32094026\n",
      "Epoch: [ 0/600] [ 133/ 276] time: 5.4527s, d_loss: 1.92545140, g_loss: 6.62253904, rnn_loss: 0.44033721\n",
      "Epoch: [ 0/600] [ 134/ 276] time: 6.0743s, d_loss: 1.45951891, g_loss: 4.97646856, rnn_loss: 0.41821718\n",
      "Epoch: [ 0/600] [ 135/ 276] time: 5.3593s, d_loss: 1.86819184, g_loss: 3.23867130, rnn_loss: 0.39704949\n",
      "Epoch: [ 0/600] [ 136/ 276] time: 5.7175s, d_loss: 2.36375713, g_loss: 4.78322220, rnn_loss: 0.41750407\n",
      "Epoch: [ 0/600] [ 137/ 276] time: 3.7000s, d_loss: 1.87547827, g_loss: 3.64671206, rnn_loss: 0.45320377\n",
      "Epoch: [ 0/600] [ 138/ 276] time: 3.5754s, d_loss: 1.56682658, g_loss: 4.01572371, rnn_loss: 0.38714689\n",
      "Epoch: [ 0/600] [ 139/ 276] time: 6.9133s, d_loss: 1.94572377, g_loss: 3.82520270, rnn_loss: 0.37597394\n",
      "Epoch: [ 0/600] [ 140/ 276] time: 4.5375s, d_loss: 2.10147357, g_loss: 3.87741303, rnn_loss: 0.35511237\n",
      "Epoch: [ 0/600] [ 141/ 276] time: 7.2692s, d_loss: 1.90975153, g_loss: 4.34906387, rnn_loss: 0.39944905\n",
      "Epoch: [ 0/600] [ 142/ 276] time: 3.6946s, d_loss: 1.45378017, g_loss: 6.58896828, rnn_loss: 0.44150376\n",
      "Epoch: [ 0/600] [ 143/ 276] time: 4.3202s, d_loss: 2.95750833, g_loss: 1.80057240, rnn_loss: 0.38762522\n",
      "Epoch: [ 0/600] [ 144/ 276] time: 5.0684s, d_loss: 2.73836994, g_loss: 8.28964424, rnn_loss: 0.38611126\n",
      "Epoch: [ 0/600] [ 145/ 276] time: 6.0129s, d_loss: 1.73702359, g_loss: 7.13402033, rnn_loss: 0.40529001\n",
      "Epoch: [ 0/600] [ 146/ 276] time: 5.1144s, d_loss: 2.22120285, g_loss: 2.74798346, rnn_loss: 0.39475062\n",
      "Epoch: [ 0/600] [ 147/ 276] time: 5.7078s, d_loss: 2.47122645, g_loss: 6.09929371, rnn_loss: 0.37772411\n",
      "Epoch: [ 0/600] [ 148/ 276] time: 6.0976s, d_loss: 2.97219872, g_loss: 3.19218493, rnn_loss: 0.31153363\n",
      "Epoch: [ 0/600] [ 149/ 276] time: 5.6159s, d_loss: 1.91357732, g_loss: 3.64046955, rnn_loss: 0.38465798\n",
      "Epoch: [ 0/600] [ 150/ 276] time: 5.2553s, d_loss: 1.89284873, g_loss: 5.77856636, rnn_loss: 0.41305155\n",
      "Epoch: [ 0/600] [ 151/ 276] time: 5.3167s, d_loss: 2.36061716, g_loss: 2.11120486, rnn_loss: 0.40896675\n",
      "Epoch: [ 0/600] [ 152/ 276] time: 4.6957s, d_loss: 3.17965913, g_loss: 4.85908604, rnn_loss: 0.36924684\n",
      "Epoch: [ 0/600] [ 153/ 276] time: 5.1425s, d_loss: 2.30677557, g_loss: 3.76924801, rnn_loss: 0.36585584\n",
      "Epoch: [ 0/600] [ 154/ 276] time: 4.3947s, d_loss: 1.44926167, g_loss: 4.82030010, rnn_loss: 0.40429020\n",
      "Epoch: [ 0/600] [ 155/ 276] time: 5.5556s, d_loss: 2.38833070, g_loss: 2.15952253, rnn_loss: 0.37143770\n",
      "Epoch: [ 0/600] [ 156/ 276] time: 4.7658s, d_loss: 2.76425934, g_loss: 6.08112240, rnn_loss: 0.34161326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0/600] [ 157/ 276] time: 5.9420s, d_loss: 2.31515121, g_loss: 2.12479353, rnn_loss: 0.32485321\n",
      "Epoch: [ 0/600] [ 158/ 276] time: 5.1188s, d_loss: 2.86995721, g_loss: 5.89485264, rnn_loss: 0.38428402\n",
      "Epoch: [ 0/600] [ 159/ 276] time: 5.4569s, d_loss: 2.00934982, g_loss: 5.71240091, rnn_loss: 0.34296393\n",
      "Epoch: [ 0/600] [ 160/ 276] time: 5.3025s, d_loss: 3.18020320, g_loss: 1.13519013, rnn_loss: 0.41942060\n",
      "Epoch: [ 0/600] [ 161/ 276] time: 4.9227s, d_loss: 2.72794867, g_loss: 5.38164520, rnn_loss: 0.34972164\n",
      "Epoch: [ 0/600] [ 162/ 276] time: 3.6922s, d_loss: 1.88686323, g_loss: 5.01856422, rnn_loss: 0.40973467\n",
      "Epoch: [ 0/600] [ 163/ 276] time: 6.1710s, d_loss: 1.82138300, g_loss: 3.77082682, rnn_loss: 0.41383132\n",
      "Epoch: [ 0/600] [ 164/ 276] time: 5.9691s, d_loss: 2.01513839, g_loss: 5.56316376, rnn_loss: 0.37942716\n",
      "Epoch: [ 0/600] [ 165/ 276] time: 4.6388s, d_loss: 2.39445162, g_loss: 2.91132379, rnn_loss: 0.35905597\n",
      "Epoch: [ 0/600] [ 166/ 276] time: 5.6247s, d_loss: 1.77917659, g_loss: 4.43264103, rnn_loss: 0.30280280\n",
      "Epoch: [ 0/600] [ 167/ 276] time: 5.6621s, d_loss: 2.05883980, g_loss: 2.05939150, rnn_loss: 0.41595364\n",
      "Epoch: [ 0/600] [ 168/ 276] time: 3.6181s, d_loss: 2.04851937, g_loss: 2.40519834, rnn_loss: 0.35652745\n",
      "Epoch: [ 0/600] [ 169/ 276] time: 6.9990s, d_loss: 1.57653522, g_loss: 5.27766323, rnn_loss: 0.43192226\n",
      "Epoch: [ 0/600] [ 170/ 276] time: 5.5098s, d_loss: 2.26969981, g_loss: 1.72937453, rnn_loss: 0.35328707\n",
      "Epoch: [ 0/600] [ 171/ 276] time: 3.6152s, d_loss: 2.40840435, g_loss: 4.60655832, rnn_loss: 0.37930292\n",
      "Epoch: [ 0/600] [ 172/ 276] time: 7.8132s, d_loss: 2.46260953, g_loss: 1.24551868, rnn_loss: 0.36947888\n",
      "Epoch: [ 0/600] [ 173/ 276] time: 5.6131s, d_loss: 2.32885623, g_loss: 4.82516193, rnn_loss: 0.38025570\n",
      "Epoch: [ 0/600] [ 174/ 276] time: 4.6184s, d_loss: 2.23427868, g_loss: 2.67572689, rnn_loss: 0.42057705\n",
      "Epoch: [ 0/600] [ 175/ 276] time: 6.5414s, d_loss: 1.75504482, g_loss: 3.46803379, rnn_loss: 0.39166820\n",
      "Epoch: [ 0/600] [ 176/ 276] time: 5.5221s, d_loss: 2.42098427, g_loss: 2.15525007, rnn_loss: 0.36883008\n",
      "Epoch: [ 0/600] [ 177/ 276] time: 4.2144s, d_loss: 2.05181122, g_loss: 3.75387526, rnn_loss: 0.40704501\n",
      "Epoch: [ 0/600] [ 178/ 276] time: 7.0658s, d_loss: 2.39391661, g_loss: 1.77405572, rnn_loss: 0.38166067\n",
      "Epoch: [ 0/600] [ 179/ 276] time: 5.1583s, d_loss: 2.01135087, g_loss: 2.65608668, rnn_loss: 0.35074848\n",
      "Epoch: [ 0/600] [ 180/ 276] time: 4.0941s, d_loss: 1.99430060, g_loss: 4.68910027, rnn_loss: 0.41128770\n",
      "Epoch: [ 0/600] [ 181/ 276] time: 6.3164s, d_loss: 2.27709270, g_loss: 1.94023919, rnn_loss: 0.36839232\n",
      "Epoch: [ 0/600] [ 182/ 276] time: 5.3659s, d_loss: 2.18890476, g_loss: 5.82291222, rnn_loss: 0.38701677\n",
      "Epoch: [ 0/600] [ 183/ 276] time: 4.1255s, d_loss: 2.25310946, g_loss: 2.57318735, rnn_loss: 0.37934881\n",
      "Epoch: [ 0/600] [ 184/ 276] time: 6.5703s, d_loss: 2.23377514, g_loss: 0.39781100, rnn_loss: 0.35927960\n",
      "Epoch: [ 0/600] [ 185/ 276] time: 5.3689s, d_loss: 3.81430602, g_loss: 5.47847271, rnn_loss: 0.35582477\n",
      "Epoch: [ 0/600] [ 186/ 276] time: 4.2060s, d_loss: 2.83370805, g_loss: 2.29831839, rnn_loss: 0.39417881\n",
      "Epoch: [ 0/600] [ 187/ 276] time: 5.8182s, d_loss: 1.97457266, g_loss: 2.05546856, rnn_loss: 0.39652663\n",
      "Epoch: [ 0/600] [ 188/ 276] time: 5.8613s, d_loss: 2.16341162, g_loss: 5.30843782, rnn_loss: 0.30085468\n",
      "Epoch: [ 0/600] [ 189/ 276] time: 3.6202s, d_loss: 2.84428620, g_loss: 2.64089298, rnn_loss: 0.39840156\n",
      "Epoch: [ 0/600] [ 190/ 276] time: 6.9172s, d_loss: 1.47272730, g_loss: 2.22125936, rnn_loss: 0.34282365\n",
      "Epoch: [ 0/600] [ 191/ 276] time: 5.1231s, d_loss: 1.86657572, g_loss: 2.27818346, rnn_loss: 0.38082874\n",
      "Epoch: [ 0/600] [ 192/ 276] time: 4.3116s, d_loss: 2.10188890, g_loss: 1.55500460, rnn_loss: 0.42090505\n",
      "Epoch: [ 0/600] [ 193/ 276] time: 6.6304s, d_loss: 2.03949785, g_loss: 4.72977304, rnn_loss: 0.34200782\n",
      "Epoch: [ 0/600] [ 194/ 276] time: 5.1920s, d_loss: 2.34459019, g_loss: 1.25700736, rnn_loss: 0.37838104\n",
      "Epoch: [ 0/600] [ 195/ 276] time: 4.0916s, d_loss: 2.23360896, g_loss: 2.71169901, rnn_loss: 0.31480175\n",
      "Epoch: [ 0/600] [ 196/ 276] time: 3.6955s, d_loss: 1.89206672, g_loss: 2.85763359, rnn_loss: 0.37485725\n",
      "Epoch: [ 0/600] [ 197/ 276] time: 4.2136s, d_loss: 2.18035626, g_loss: 1.41938627, rnn_loss: 0.37685424\n",
      "Epoch: [ 0/600] [ 198/ 276] time: 6.1852s, d_loss: 2.61968780, g_loss: 5.32005024, rnn_loss: 0.34518522\n",
      "Epoch: [ 0/600] [ 199/ 276] time: 5.2893s, d_loss: 2.70548391, g_loss: 2.09836674, rnn_loss: 0.37686765\n",
      "Epoch: [ 0/600] [ 200/ 276] time: 4.1133s, d_loss: 2.00068593, g_loss: 2.71451569, rnn_loss: 0.32919514\n",
      "Epoch: [ 0/600] [ 201/ 276] time: 6.2460s, d_loss: 2.17722178, g_loss: 3.51018715, rnn_loss: 0.36967206\n",
      "Epoch: [ 0/600] [ 202/ 276] time: 5.0338s, d_loss: 1.79588866, g_loss: 3.44847226, rnn_loss: 0.32412302\n",
      "Epoch: [ 0/600] [ 203/ 276] time: 4.3363s, d_loss: 1.83768868, g_loss: 3.41627908, rnn_loss: 0.33518535\n",
      "Epoch: [ 0/600] [ 204/ 276] time: 6.0556s, d_loss: 2.02145529, g_loss: 1.91596675, rnn_loss: 0.31537709\n",
      "Epoch: [ 0/600] [ 205/ 276] time: 5.0808s, d_loss: 2.00664258, g_loss: 5.20211697, rnn_loss: 0.39659667\n",
      "Epoch: [ 0/600] [ 206/ 276] time: 3.5902s, d_loss: 2.45127678, g_loss: 1.19467092, rnn_loss: 0.36465073\n",
      "Epoch: [ 0/600] [ 207/ 276] time: 6.9198s, d_loss: 2.56745434, g_loss: 4.61995506, rnn_loss: 0.35611710\n",
      "Epoch: [ 0/600] [ 208/ 276] time: 5.1816s, d_loss: 2.12142801, g_loss: 2.44006777, rnn_loss: 0.34937781\n",
      "Epoch: [ 0/600] [ 209/ 276] time: 4.2222s, d_loss: 1.75089884, g_loss: 2.42467594, rnn_loss: 0.35942951\n",
      "Epoch: [ 0/600] [ 210/ 276] time: 7.0136s, d_loss: 1.82422996, g_loss: 4.17180347, rnn_loss: 0.30216518\n",
      "Epoch: [ 0/600] [ 211/ 276] time: 5.3455s, d_loss: 1.61179280, g_loss: 1.80976057, rnn_loss: 0.34374779\n",
      "Epoch: [ 0/600] [ 212/ 276] time: 4.1631s, d_loss: 2.31790709, g_loss: 3.71578932, rnn_loss: 0.37295866\n",
      "Epoch: [ 0/600] [ 213/ 276] time: 6.1426s, d_loss: 1.67943263, g_loss: 3.34744596, rnn_loss: 0.33302301\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "sess = tf.Session(config=tf.ConfigProto())\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=100)\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=5)\n",
    "checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "'''if ckpt and ckpt.model_checkpoint_path:\n",
    "    loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "    load_step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "else:\n",
    "    print('no checkpoints find.')'''\n",
    "\n",
    "n_epoch = cfg.TRAIN.MAX_EPOCH\n",
    "n_images_train = len(train_dataset.images)\n",
    "n_batch_epoch = int(n_images_train / batch_size)\n",
    "train_captions = np.array(train_dataset.captions_ids)\n",
    "train_images = np.array(train_dataset.images)\n",
    "n_captions_train = len(train_captions)\n",
    "n_captions_per_image = cfg.TEXT.CAPTIONS_PER_IMAGE\n",
    "\n",
    "for epoch in range(cfg.TRAIN.MAX_EPOCH):\n",
    "    #################################################\n",
    "    # TODO: Implement text to image synthesis\n",
    "    start_time = time.time()\n",
    "\n",
    "    if epoch !=0 and (epoch % decay_every == 0):\n",
    "        new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "        sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
    "        log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "        print(log)\n",
    "\n",
    "    elif epoch == 0:\n",
    "        log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "        print(log)\n",
    "    \n",
    "    for step in range(n_batch_epoch):\n",
    "        step_time = time.time()\n",
    "\n",
    "        ## get matched text & image\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_real_caption = train_captions[idexs]\n",
    "        b_real_images = train_images[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n",
    "\n",
    "        ## get wrong caption & wrong image\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_wrong_caption = train_captions[idexs]\n",
    "        idexs2 = get_random_int(min=0, max=n_images_train-1, number=batch_size)\n",
    "        b_wrong_images = train_images[idexs2]\n",
    "\n",
    "        ## get noise\n",
    "        b_z = np.random.normal(loc=0.0, scale=1.0, size=(batch_size, z_dim)).astype(np.float32)\n",
    "\n",
    "        b_real_images = threading_data(b_real_images, prepro_img, mode='train')   # [0, 255] --> [-1, 1] + augmentation\n",
    "        b_wrong_images = threading_data(b_wrong_images, prepro_img, mode='train')\n",
    "\n",
    "        ## update RNN\n",
    "        if epoch < int(n_epoch/5):\n",
    "            errRNN, _ = sess.run([rnn_loss, rnn_optim], feed_dict={\n",
    "                                            t_real_image : b_real_images,\n",
    "                                            t_wrong_image : b_wrong_images,\n",
    "                                            t_real_caption : b_real_caption,\n",
    "                                            t_wrong_caption : b_wrong_caption})\n",
    "        else:\n",
    "            errRNN = 0\n",
    "\n",
    "        ## updates D\n",
    "        errD, _ = sess.run([d_loss, d_optim], feed_dict={\n",
    "                        t_real_image : b_real_images,\n",
    "                        t_wrong_caption : b_wrong_caption,\n",
    "                        t_real_caption : b_real_caption,\n",
    "                        t_z : b_z})\n",
    "        ## updates G\n",
    "        errG, _ = sess.run([g_loss, g_optim], feed_dict={\n",
    "                        t_real_caption : b_real_caption,\n",
    "                        t_z : b_z})\n",
    "\n",
    "        print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.8f, g_loss: %.8f, rnn_loss: %.8f\" \\\n",
    "                    % (epoch, n_epoch, step, n_batch_epoch, time.time() - step_time, errD, errG, errRNN))\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\" ** Epoch %d took %fs\" % (epoch, time.time()-start_time))\n",
    "        '''img_gen, rnn_out = sess.run([net_g.outputs, net_rnn.outputs], feed_dict={\n",
    "                                    t_real_caption : sample_sentence,\n",
    "                                    t_z : sample_seed})\n",
    "\n",
    "        save_images(img_gen, [ni, ni], 'train_samples/train_{:02d}.png'.format(epoch))'''\n",
    "\n",
    "    if (epoch != 0) and (epoch % 10) == 0:\n",
    "        #################################################\n",
    "        # save checkpoints\n",
    "        checkpoint_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "        saver.save(sess, checkpoint_path, global_step=epoch)\n",
    "        print('The checkpoint has been created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r_precision_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids), (-1, cfg.TEXT.WORDS_NUM))\n",
    "    captions_ids_wrong = np.reshape(test_dataset.random_wrong_captions(), (-1, cfg.WRONG_CAPTION, cfg.TEXT.WORDS_NUM))\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # load the trained checkpoint\n",
    "    cfg.CHECKPOINT_DIR = 'checkpoint'\n",
    "    cfg.CHECKPOINT_NAME = 'model.ckpt-430'\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    true_cnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    true_rnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    wrong_rnn_features = np.zeros((num_batches, cfg.WRONG_CAPTION, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "        z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "        \n",
    "        rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap})\n",
    "        gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "        cnn_features = sess.run(cnn_encoder.outputs, feed_dict={t_real_image: gen})\n",
    "\n",
    "        true_cnn_features[i] = cnn_features\n",
    "        true_rnn_features[i] = rnn_features\n",
    "\n",
    "        for per_wrong_caption in range(cfg.WRONG_CAPTION):\n",
    "            test_cap = captions_ids_wrong[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "            rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap[:, per_wrong_caption]})\n",
    "            wrong_rnn_features[i, per_wrong_caption] = rnn_features\n",
    "    \n",
    "    # if exists, remove the existing file first\n",
    "    try:\n",
    "        os.remove(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE))\n",
    "    except OSError:\n",
    "        pass\n",
    "    np.savez(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE), true_cnn=true_cnn_features, true_rnn=true_rnn_features,\n",
    "             wrong_rnn=wrong_rnn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inception_score_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids),\n",
    "                             (-1, cfg.TEXT.CAPTIONS_PER_IMAGE, cfg.TEXT.WORDS_NUM))\n",
    "    \n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        for per_caption in range(cfg.TEXT.CAPTIONS_PER_IMAGE):\n",
    "            test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE, per_caption]\n",
    "            test_directory = test_dataset.filenames[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "            z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "            gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "            \n",
    "            for j in range(cfg.BATCH_SIZE):\n",
    "                if not os.path.exists(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0])):\n",
    "                    os.mkdir(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0]))\n",
    "\n",
    "                scipy.misc.imsave(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j] + '_{}.png'.format(per_caption)), gen[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/model.ckpt-430\n",
      "Restored model parameters from checkpoint/model.ckpt-430\n"
     ]
    }
   ],
   "source": [
    "generate_r_precision_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/model.ckpt-430\n",
      "Restored model parameters from checkpoint/model.ckpt-430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duclv/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "generate_inception_score_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measure Inception score and R-precision of given test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After set the config file as 'eval_birds.yml' and run the 'generate_inception_score_data()' and 'generate_r_precision_data()', the synthesized images based on given captions and set of image and caption features should be saved inside a 'evaluation' folder, specifically in 'evaluation/generated_images/..' and as 'evaluation/r_precision.npz' respectively.\n",
    "\n",
    "**Then, go to the 'evaluation' folder and run each 'inception_score.ipynb' and 'r_precision.ipynb' file in order to measure inception score and r-precision score.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-19] *",
   "language": "python",
   "name": "conda-env-deep-learning-19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
